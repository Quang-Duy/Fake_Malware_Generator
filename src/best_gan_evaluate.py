""" Evaluate and save best GAN model among save_intervals

Usage

>>> python -m src.best_gan_evaluate

"""
import os

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import logging
import coloredlogs
import hydra
import mlflow
import numpy as np
import omegaconf
from omegaconf import DictConfig
from tensorflow.keras.layers import LeakyReLU
from tensorflow.keras.models import load_model
from src.common.utils import PROJECT_ROOT, MyTimer

logger = logging.getLogger(__name__)
coloredlogs.install(level=logging.DEBUG, logger=logger)

logging.getLogger('git').setLevel(logging.ERROR)
logging.getLogger('urllib3').setLevel(logging.ERROR)
logging.getLogger('tensorflow').setLevel(logging.ERROR)
logging.getLogger('h5py').setLevel(logging.ERROR)

logger.propagate = False


def log_params(cfg: DictConfig) -> None:
    # Log Hydra configs parameters
    mlflow.log_params(cfg.data.datamodule.datasets.train)
    mlflow.log_param(key='latent_dim',
                     value=cfg.model.modelmodule.model.latent_dim)

    for key, params in cfg.gan_evaluate.evalmodule.items():
        if key == '_target_' or key == 'eval_experiment_name' or \
                key == 'eval_run_name':
            mlflow.log_param(key=key, value=params)
            continue

        mlflow.log_params(params)

    if cfg.classifiers.clf_module.ocsvm_param.usage:
        mlflow.log_params(cfg.classifiers.clf_module.ocsvm_param)
    elif cfg.classifiers.clf_module.isoForest_param.usage:
        mlflow.log_params(cfg.classifiers.clf_module.isoForest_param)
    elif cfg.classifiers.clf_module.lof_param.usage:
        mlflow.log_params(cfg.classifiers.clf_module.lof_param)

    # Log hydra config files
    mlflow.log_artifacts(os.path.join(PROJECT_ROOT, 'conf'))

    # Log python scripts
    mlflow.log_artifact(
        os.path.join(PROJECT_ROOT, 'src/best_gan_evaluate.py')
    )
    mlflow.log_artifact(
        os.path.join(PROJECT_ROOT, 'src/data/data_module.py')
    )


class GANEvaluator:
    """ Contains GAN evaluators: SVM, Random Forest, GaussianNB"""

    def __init__(self, param: DictConfig,
                 modelmodule: DictConfig,
                 *args, **kwargs
                 ) -> None:
        self.eval_param = param
        self.modelmodule = modelmodule

    def make_train_data_with_label(self, real_data: np.ndarray,
                                   gen_samples: np.ndarray):
        idx = np.random.randint(
            low=0, high=real_data.shape[0], size=self.eval_param.batch_size
        )
        real = real_data[idx]

        train_data = np.concatenate((real, gen_samples))
        label = np.array(
            [1] * self.eval_param.batch_size + [0] * self.eval_param.batch_size
        )

        return train_data, label

    def load_saved_model(self, model_num_epoch: int):
        save_interval = 'save_interval_bert'
        path = os.path.join(PROJECT_ROOT, self.eval_param.models_path,
                            save_interval)
        file_name = str(self.eval_param.architecture + '_' +
                        self.eval_param.family + '_' +
                        str(model_num_epoch) + '_bert.hdf5')
        path_to_model = path + '/' + file_name

        if not os.path.exists(path):
            raise FileNotFoundError(
                f"Model path {path_to_model} was not found. "
                f"Current dir: {os.getcwd()}")

        generator = load_model(
            path_to_model, custom_objects={'LeakyReLU': LeakyReLU}
        )
        return generator

    def generate(self, generator) -> np.ndarray:
        random_normal_size = (self.eval_param.batch_size,
                              self.modelmodule.model.latent_dim)
        noise = np.random.normal(0, 1, size=random_normal_size)
        gen_samples = generator.predict(noise)

        new_shape = (self.eval_param.batch_size,
                     self.modelmodule.model.num_neurons)
        gen_samples = np.reshape(gen_samples, newshape=new_shape)
        return gen_samples

    def evaluate(self, clf, real_data: np.ndarray, num_of_sub_batches):
        initial = self.eval_param.initial

        # Loop through each Generator
        while initial <= self.eval_param.limit:
            # Get ordinal numbers of the current Generator
            model_nth = initial / self.eval_param.increment

            # Load up the Generator
            generator = self.load_saved_model(initial)

            # Generate 10 sub-batches and combine them together into a test set
            X_test = np.array([]).reshape(0, 768)
            for _ in range(num_of_sub_batches):
                gen_samples = self.generate(generator)
                X_test = np.vstack([X_test, gen_samples])

            mean_test_acc = clf.cross_validation(
                train_data=real_data, test_data=X_test, model_nth=model_nth
            )
            logger.debug(
                f"Gen {int(model_nth)}th - Mean Test Accuracy: {mean_test_acc}"
            )

            initial += self.eval_param.increment


def evaluate_wgangp(cfg: omegaconf.DictConfig, real_data):
    # logger.info('Loading dataset ...')
    # datamodule = hydra.utils.instantiate(
    #     cfg.data.datamodule, _recursive_=False
    # )
    # data = datamodule.prepare_data()
    # # data = data[:200]
    #
    # distil_bert = hydra.utils.instantiate(
    #     cfg.bert.bertmodule, _recursive_=False
    # )
    # logger.info('Embedding dataset ...')
    # real_data = distil_bert.embed(data)
    #
    # # Scale to [-1, 1]
    # min_val = np.amin(real_data)
    # max_val = np.amax(real_data)
    # real_data = 2 * (real_data - min_val) / (max_val - min_val) - 1

    evaluator = hydra.utils.instantiate(
        cfg.gan_evaluate.evalmodule,
        modelmodule=cfg.model.modelmodule,
        _recursive_=False
    )

    clf = hydra.utils.instantiate(
        cfg.classifiers.clf_module,
        _recursive_=False
    )

    logger.info('Evaluating ...')
    os.chdir(PROJECT_ROOT)
    mlflow.set_experiment(cfg.gan_evaluate.evalmodule.eval_experiment_name)

    logger.info('Beginning experiment '
                f'{cfg.gan_evaluate.evalmodule.eval_experiment_name} ...')

    num_of_sub_batches = cfg.gan_evaluate.evalmodule.param.num_of_sub_batches
    with mlflow.start_run(run_name=cfg.gan_evaluate.evalmodule.eval_run_name):
        log_params(cfg)
        mlflow.log_param(key='Dataset size', value=len(real_data))

        my_timer = MyTimer()
        evaluator.evaluate(
            clf, real_data, num_of_sub_batches=num_of_sub_batches
        )

    logger.info('Finished evaluating! --- '
                f'Execution time: {my_timer.get_execution_time()}')

    logger.info('Finished Operation!!')


@hydra.main(config_path=str(PROJECT_ROOT / 'conf'), config_name='default')
def main(cfg: omegaconf.DictConfig):
    evaluate_wgangp(cfg)


if __name__ == '__main__':
    logger.propagate = False
    main()
