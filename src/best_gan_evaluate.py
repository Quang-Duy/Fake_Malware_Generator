""" Evaluate and save best GAN model among save_intervals

Usage

>>> python -m src.best_gan_evaluate

"""
import os

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import logging
import coloredlogs
import hydra
import mlflow
import numpy as np
import omegaconf
from omegaconf import DictConfig
from tensorflow.keras.models import load_model
from src.common.utils import PROJECT_ROOT, MyTimer
from src.model.classifiers import Classifiers

logging.getLogger('git').setLevel(logging.ERROR)
logging.getLogger('urllib3').setLevel(logging.ERROR)
logging.getLogger('tensorflow').setLevel(logging.ERROR)
logging.getLogger('h5py').setLevel(logging.ERROR)
logger = logging.getLogger(__name__)
coloredlogs.install(level=logging.DEBUG, logger=logger)


def log_params(cfg: DictConfig) -> None:
    # Log Hydra configs parameters
    mlflow.log_params(cfg.data.datamodule.datasets.train)
    mlflow.log_param(key='latent_dim',
                     value=cfg.model.modelmodule.model.latent_dim)

    for key, params in cfg.gan_evaluate.evalmodule.items():
        if key == '_target_' or key == 'eval_experiment_name' or \
                key == 'eval_run_name':
            mlflow.log_param(key=key, value=params)
            continue

        mlflow.log_params(params)

    if cfg.classifiers.clf_module.svm_param.usage:
        mlflow.log_params(cfg.classifiers.clf_module.svm_param)
    elif cfg.classifiers.clf_module.rf_param.usage:
        mlflow.log_params(cfg.classifiers.clf_module.rf_param)
    elif cfg.classifiers.clf_module.naive_bayes_param.usage:
        mlflow.log_params(cfg.classifiers.clf_module.naive_bayes_param)
    elif cfg.classifiers.clf_module.knn_param.usage:
        mlflow.log_params(cfg.classifiers.clf_module.knn_param)

    # Log hydra config files
    mlflow.log_artifacts(os.path.join(PROJECT_ROOT, 'conf'))

    # Log python scripts
    mlflow.log_artifact(
        os.path.join(PROJECT_ROOT, 'src/best_gan_evaluate.py')
    )
    mlflow.log_artifact(
        os.path.join(PROJECT_ROOT, 'src/data/data_module.py')
    )


class GANEvaluator:
    """ Contains GAN evaluators: SVM, Random Forest, GaussianNB"""

    def __init__(self, param: DictConfig,
                 modelmodule: DictConfig,
                 *args, **kwargs
                 ) -> None:
        self.eval_param = param
        self.modelmodule = modelmodule

    def make_train_data_with_label(self, real_data: np.ndarray,
                                   gen_samples: np.ndarray):
        idx = np.random.randint(
            low=0, high=real_data.shape[0], size=self.eval_param.batch_size
        )
        real = real_data[idx]

        train_data = np.concatenate((real, gen_samples))
        label = np.array(
            [1] * self.eval_param.batch_size + [0] * self.eval_param.batch_size
        )

        return train_data, label

    def load_saved_model(self, model_num_epoch: int):
        save_interval = 'save_interval_bert'
        path = os.path.join(PROJECT_ROOT, self.eval_param.models_path,
                            save_interval)
        file_name = str(self.eval_param.architecture + '_' +
                        self.eval_param.family + '_' +
                        str(model_num_epoch) + '_bert.hdf5')
        path_to_model = path + '/' + file_name

        if not os.path.exists(path):
            raise FileNotFoundError(
                f"Model path {path_to_model} was not found. "
                f"Current dir: {os.getcwd()}")

        generator = load_model(path_to_model)
        return generator

    def generate(self, generator) -> np.ndarray:
        random_normal_size = (self.eval_param.batch_size,
                              self.modelmodule.model.latent_dim)
        noise = np.random.normal(0, 1, size=random_normal_size)
        gen_samples = generator.predict(noise)

        new_shape = (self.eval_param.batch_size,
                     self.modelmodule.model.num_neurons)
        gen_samples = np.reshape(gen_samples, newshape=new_shape)
        return gen_samples

    def evaluate(self, clf, real_data: np.ndarray, evaluator_nth):
        initial = self.eval_param.initial
        epoch_map = np.zeros(self.eval_param.total_models)
        index = 0

        while initial <= self.eval_param.limit:
            model_nth = initial / self.eval_param.increment
            generator = self.load_saved_model(initial)
            gen_samples = self.generate(generator)

            train_data, label = self.make_train_data_with_label(
                real_data=real_data, gen_samples=gen_samples
            )

            mean_acc = clf.train_and_eval(
                train_data=train_data, label=label,
                model_nth=model_nth, evaluator_nth=evaluator_nth
            )
            epoch_map[index] += mean_acc

            index += 1
            initial += self.eval_param.increment
        return epoch_map


@hydra.main(config_path=str(PROJECT_ROOT / 'conf'), config_name='default')
def main(cfg: omegaconf.DictConfig):
    logger.info('Loading dataset ...')
    datamodule = hydra.utils.instantiate(
        cfg.data.datamodule, _recursive_=False
    )
    data = datamodule.prepare_data()
    data = data[:200]

    distil_bert = hydra.utils.instantiate(
        cfg.bert.bertmodule, _recursive_=False
    )
    logger.info('Embedding dataset ...')
    real_data = distil_bert.embed(data)

    # Scale to [-1, 1]
    min_val = np.amin(real_data)
    max_val = np.amax(real_data)
    real_data = 2 * (real_data - min_val) / (max_val - min_val) - 1

    evaluators = [
        hydra.utils.instantiate(
            cfg.gan_evaluate.evalmodule,
            modelmodule=cfg.model.modelmodule,
            _recursive_=False
        ) for _ in range(5)
    ]

    clf = hydra.utils.instantiate(
        cfg.classifiers.clf_module,
        _recursive_=False
    )

    logger.info('Evaluating ...')
    os.chdir(PROJECT_ROOT)
    mlflow.set_experiment(cfg.gan_evaluate.evalmodule.eval_experiment_name)

    logger.info('Beginning experiment '
                f'{cfg.gan_evaluate.evalmodule.eval_experiment_name} ...')

    with mlflow.start_run(run_name=cfg.gan_evaluate.evalmodule.eval_run_name):
        log_params(cfg)
        mlflow.log_param(key='Dataset size', value=len(real_data))

        my_timer = MyTimer()
        total = np.zeros(cfg.gan_evaluate.evalmodule.param.total_models)
        for count, evaluator in enumerate(evaluators):
            logger.info(f'Evaluator #{count + 1} ...')
            total += evaluator.evaluate(clf, real_data, count)

        logger.info('Finished evaluating! '
                    f'Execution time: {my_timer.get_execution_time()}')
        logger.debug(f'Model scores after {len(evaluators)} evaluation \n'
                     f'{total}')
        total /= 5

        logger.info('Best Gen: {} -- Accuracy: {:.2f}'
                    .format(np.argmin(total), np.min(total)))
        for count, score in enumerate(total):
            mlflow.log_metric(
                key=f'Avg_Score_for_Gen_{count}', value=score
            )

    logger.info('Finished Operation!!')


if __name__ == '__main__':
    logger.propagate = False
    main()
