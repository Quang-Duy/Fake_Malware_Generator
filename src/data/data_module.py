""" Create Data Module

Usage

>>> python -m src.data.data_module

"""

import glob
import json
import logging
import os

import coloredlogs
import hydra
import numpy as np
import omegaconf
from omegaconf import DictConfig

from src.common.utils import PROJECT_ROOT, MyTimer

logger = logging.getLogger(__name__)
coloredlogs.install(level=logging.DEBUG, logger=logger)


class MyDataModule:
    """Build data module"""

    def __init__(self, datasets: DictConfig) -> None:
        """ Initialize data variables from hydra config

        Args:
            datasets (DictConfig): dataset contains train
        """
        self.datasets = datasets
        self.max_file_len = self.datasets.train.max_file_len
    '''
        def load_data(self):
        """ Load data from hydra config

        Returns:
            data (np.array): array size (number of files in family, longest
                             number of words in a file) contains opcodes
                             mapped to integers
            len_mapping (int): number of unique opcodes
        """
        # Load .json file that contains mapping opcodes to integer
        opdict_path = os.path.join(PROJECT_ROOT,
                                   self.datasets.train.opdict_path,
                                   f'opdict_{self.datasets.train.family}'
                                   '.json')

        with open(opdict_path, 'r') as json_file:
            mapping = json.load(json_file)

        path = os.path.join(PROJECT_ROOT,
                            self.datasets.train.raw_path,
                            self.datasets.train.family)
        if not os.path.exists(path):
            raise FileNotFoundError(
                f"Path {path} was not found. "
                f"Current dir: {os.getcwd()}")

        data = []
        path = os.path.join(path, '*.txt')
        files = glob.glob(path)
        logger.info(f'Loading {self.datasets.train.family} --- '
                    f'Number of files found: {len(files)}')

        # Loop through each .txt file to load and map opcodes to integer
        for file in files:
            with open(file, 'r') as rf:
                content = rf.read().split()

            num_words = len(content)
            current_file = []

            # === Loop through `content` === #
            # Add `-1` (aka white space) at the end if the file does not
            # have enough `max_file_len` words
            for i in range(self.max_file_len):
                if i <= num_words - 1:

                    # If opcode is not in the top k (k different depend on
                    # each family), opcode will be map to integer k+1
                    if content[i] in mapping:
                        current_file.append(mapping[content[i]])
                    else:
                        current_file.append(len(mapping) - 1)
                else:
                    current_file.append(mapping.get(' '))

            data.append(current_file)

        return np.array(data), len(mapping) - 1
    '''

    def load_data(self, tokenized: bool = True):
        opdict_path = os.path.join(PROJECT_ROOT,
                                   self.datasets.train.opdict_path,
                                   f'opdict_{self.datasets.train.family}'
                                   '.json')

        with open(opdict_path, 'r') as json_file:
            mapping = json.load(json_file)

        path = os.path.join(PROJECT_ROOT,
                            self.datasets.train.raw_path,
                            self.datasets.train.family)
        if not os.path.exists(path):
            raise FileNotFoundError(
                f"Path {path} was not found. "
                f"Current dir: {os.getcwd()}")

        data = []
        path = os.path.join(path, '*.txt')
        files = glob.glob(path)
        logger.info(f'Loading {self.datasets.train.family} --- '
                    f'Number of files found: {len(files)}')

        # Loop through each .txt file to load and, if tokenized = True, map
        # opcodes to integer
        data = []
        for file in files:
            with open(file, 'r') as rf:
                content = rf.read().split()

            for i in range(len(content)):
                if tokenized:
                    if content[i] in mapping:
                        data.append(mapping[content[i]])
                    else:
                        data.append(len(mapping) - 1)
                else:
                    data.append(content[i])

        # Calculate data size that is divisible by `max_file_len` to reshape
        mod = len(data) % self.datasets.train.max_file_len
        reshape_size = len(data) - mod

        data = np.reshape(
            data[:reshape_size], (-1, self.datasets.train.max_file_len)
        )

        return np.array(data), len(mapping) - 1


@hydra.main(config_path=str(PROJECT_ROOT / "conf"), config_name="default")
def main(cfg: omegaconf.DictConfig):
    logger.propagate = False

    # Start timer
    my_timer = MyTimer()

    datamodule = hydra.utils.instantiate(cfg.data.datamodule,
                                         _recursive_=False)

    sample, num_unique = datamodule.load_data()
    print(f"Number of unique opcodes: {num_unique}")
    print(f"Execution Time: {my_timer.get_execution_time()}")


if __name__ == '__main__':
    main()
