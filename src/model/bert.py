""" Create Distil-BERT for feature extraction

Usage

>>> python -m src.model.bert

"""

import hydra
import numpy as np
import omegaconf
import torch
import transformers as ppb

from src.common.utils import PROJECT_ROOT


class DistilBERT:
    def __init__(self, param):
        """ Initialize Distil-BERT model

        Args:
            param: distil-bert parameters from hydra config
        """
        self.param = param
        self.model_class = ppb.DistilBertModel
        self.tokenizer_class = ppb.DistilBertTokenizer

        self.tokenizer = self.tokenizer_class.from_pretrained(
            self.param.pretrained_weights
        )
        self.model = self.model_class.from_pretrained(
            self.param.pretrained_weights
        )

    def embed(self, sample):
        tokens = self.tokenizer(
            sample, add_special_tokens=True, padding=True, truncation=True,
            max_length=self.param.max_tokens, return_tensors='pt')

        with torch.no_grad():
            last_hidden_states = self.model(
                tokens['input_ids'], attention_mask=tokens['attention_mask']
            )

        features = last_hidden_states[0][:, 0, :].numpy()
        return features


@hydra.main(config_path=str(PROJECT_ROOT / "conf"), config_name="default")
def main(cfg: omegaconf.DictConfig):
    distil_bert = hydra.utils.instantiate(
        cfg.bert.bertmodule, _recursive_=False
    )
    print("Success!") if distil_bert else print("Fail!")


if __name__ == '__main__':
    main()
