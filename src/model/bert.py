""" Create Distil-BERT for feature extraction

Usage

>>> python -m src.model.bert

"""

import hydra
import numpy as np
import omegaconf
import torch
import transformers as ppb

from src.common.utils import PROJECT_ROOT


class DistilBERT:
    def __init__(self, param):
        """ Initialize Distil-BERT model

        Args:
            param: distil-bert parameters from hydra config
        """
        self.param = param
        self.model_class = ppb.DistilBertModel
        self.tokenizer_class = ppb.DistilBertTokenizer

        self.tokenizer = self.tokenizer_class.from_pretrained(
            self.param.pretrained_weights
        )
        self.model = self.model_class.from_pretrained(
            self.param.pretrained_weights
        )
    '''
    def embed(self, sample):
    max_count = 200
    count = 1
    model_tokens = []
    for opcode_list in sample:
        # Convert opcode list into sentence to feed in BERT tokenizer
        sentence = ' '.join(opcode_list)
        tokenized = self.tokenizer.encode(
            sentence, add_special_tokens=True)
        model_tokens.append(tokenized[:400] + [102])
        count += 1
        if count > max_count:
            break

    model_tokens = np.array(model_tokens)
    attention_mask = np.where(model_tokens != 0, 1, 0)
    input_ids = torch.tensor(model_tokens).to(torch.long)
    attention_mask = torch.tensor(attention_mask)

    with torch.no_grad():
        last_hidden_states = self.model(input_ids,
                                        attention_mask=attention_mask)

    features = last_hidden_states[0][:, 0, :].numpy()
    return features
    '''
    def embed(self, sample):
        model_tokens = []
        for opcode_list in sample:
            tokens = []

            for opcode in opcode_list:
                tokenized = self.tokenizer.encode(opcode,
                                                  add_special_tokens=True)
                tokens = np.concatenate((tokens, tokenized), axis=None).astype(
                    int)

                if len(tokens) >= self.param.max_tokens:
                    tokens = tokens[:self.param.max_tokens]
                    model_tokens.append(tokens)
                    break

        model_tokens = np.array(model_tokens)
        attention_mask = np.where(model_tokens != 0, 1, 0)
        input_ids = torch.tensor(model_tokens).to(torch.long)
        attention_mask = torch.tensor(attention_mask)

        with torch.no_grad():
            last_hidden_states = self.model(input_ids,
                                            attention_mask=attention_mask)

        features = last_hidden_states[0][:, 0, :].numpy()
        return features


@hydra.main(config_path=str(PROJECT_ROOT / "conf"), config_name="default")
def main(cfg: omegaconf.DictConfig):
    distil_bert = hydra.utils.instantiate(
        cfg.bert.bertmodule, _recursive_=False
    )
    print("Success!") if distil_bert else print("Fail!")


if __name__ == '__main__':
    main()
