import logging
import coloredlogs
import hydra
import numpy as np
import mlflow
import omegaconf
from sklearn.svm import OneClassSVM
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV

from src.common.utils import PROJECT_ROOT

logger = logging.getLogger(__name__)
coloredlogs.install(level=logging.DEBUG, logger=logger)


class Classifiers:
    def __init__(self, ocsvm_param, isoForest_param,
                 lof_param, k_fold, random_state):
        self.ocsvm_param = ocsvm_param
        self.isoForest_param = isoForest_param
        self.lof_param = lof_param
        self.random_state = random_state

        self.kf = KFold(n_splits=k_fold, shuffle=True,
                        random_state=self.random_state)

    def select_classifier(self):
        clf = None
        if self.ocsvm_param.usage:
            clf = OneClassSVM(
                nu=self.ocsvm_param.nu,
                kernel=self.ocsvm_param.kernel,
                gamma=self.ocsvm_param.gamma
            )
        elif self.isoForest_param.usage:
            clf = IsolationForest(
                n_estimators=self.isoForest_param.n_estimators,
                contamination=self.isoForest_param.contamination,
                bootstrap=self.isoForest_param.bootstrap,
                max_samples=self.isoForest_param.max_samples,
                random_state=self.random_state
            )
        elif self.lof_param.usage:
            clf = LocalOutlierFactor(
                n_neighbors=self.lof_param.n_neighbors,
                algorithm=self.lof_param.algorithm,
                leaf_size=self.lof_param.leaf_size,
                p=self.lof_param.p,
                contamination=self.lof_param.contamination,
                novelty=self.lof_param.novelty
            )
        return clf

    def train_classifier(self, clf, X_train):
        """ Train and Evaluate a Classifier of choice on real and fake data

        Args:
            train_data:
            label:
            model_nth:
            evaluator_nth:

        Returns:

        """
        clf.fit(X_train)

        train_pred = clf.predict(X_train)
        train_targets = np.array([1] * len(X_train))
        train_acc = accuracy_score(train_targets, train_pred)

        return clf, train_acc

    def evaluate_classifier(self, clf, X_test):
        test_pred = clf.predict(X_test)
        test_targets = np.array([-1] * len(X_test))
        test_acc = accuracy_score(test_targets, test_pred)

        return test_acc

    def cross_validation(self, train_data, test_data, model_nth):
        train_acc = []
        val_acc = []
        test_acc = []
        step = 0

        for train_index, val_index in self.kf.split(train_data):
            X_train, X_val = train_data[train_index], train_data[val_index]

            clf = self.select_classifier()
            clf.fit(X_train)

            # Training Accuracy
            train_pred = clf.predict(X_train)
            train_targets = np.array([1] * len(X_train))
            train_score = accuracy_score(train_targets, train_pred)
            train_acc.append(train_score)
            mlflow.log_metric(key=f'Gen_{int(model_nth)}_train_accuracy',
                              value=train_score, step=step)

            # Validation Accuracy
            val_pred = clf.predict(X_val)
            val_targets = np.array([1] * len(X_val))
            val_score = accuracy_score(val_targets, val_pred)
            val_acc.append(val_score)
            mlflow.log_metric(key=f'Gen_{int(model_nth)}_val_accuracy',
                              value=val_score, step=step)

            # Test Accuracy
            idx = np.random.randint(
                low=0, high=test_data.shape[0], size=32
            )
            X_test = test_data[idx]
            test_pred = clf.predict(X_test)
            test_targets = np.array([-1] * len(X_test))
            test_score = accuracy_score(test_targets, test_pred)
            test_acc.append(test_score)
            mlflow.log_metric(key=f'Gen_{int(model_nth)}_test_accuracy',
                              value=test_score, step=step)

            step += 1

        mean_train_acc = np.mean(np.array(train_acc))
        mean_val_acc = np.mean(np.array(val_acc))
        mean_test_acc = np.mean(np.array(test_acc))

        mlflow.log_metric(
            key=f'Gen_{int(model_nth)}_mean_train_accuracy',
            value=mean_train_acc
        )

        mlflow.log_metric(
            key=f'Gen_{int(model_nth)}_mean_val_accuracy',
            value=mean_val_acc
        )

        mlflow.log_metric(
            key=f'Gen_{int(model_nth)}_mean_test_accuracy',
            value=mean_test_acc
        )
        return mean_test_acc

    def tuning_parameters(self, clf, param_grid, X_train, y_train):
        """Fine-tuning parameter for a given model

        Args:
            clf: classifier model
            param_grid: param of that given model
            X: BERT embeddings of train feature(s)
            y: label of train set

        Returns:
            the best estimator
        """
        grid_search = GridSearchCV(clf, param_grid, cv=5, n_jobs=-1,
                                   scoring='accuracy')
        grid_search.fit(X_train, y_train)

        return grid_search.best_params_


@hydra.main(config_path=str(PROJECT_ROOT / 'conf'), config_name='default')
def main(cfg: omegaconf.DictConfig):
    clf = hydra.utils.instantiate(
        cfg.classifiers.clf_module,
        _recursive_=False
    )
    print("Success!") if clf else print("Fail!")


if __name__ == "__main__":
    logger.propagate = False
    main()
