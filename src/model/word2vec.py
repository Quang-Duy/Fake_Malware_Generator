import logging
import os

import coloredlogs
import gensim
import hydra
import numpy as np
import omegaconf
from omegaconf import DictConfig

from src.common.utils import PROJECT_ROOT

logging.getLogger('gensim').setLevel(logging.ERROR)
logging.getLogger('smart_open').setLevel(logging.ERROR)
logger = logging.getLogger(__name__)
coloredlogs.install(level=logging.DEBUG, logger=logger)


class Word2Vec:
    def __init__(self, param: DictConfig, real_samples, batch_size):
        self.word2vec_param = param
        self.real_samples = real_samples
        self.batch_size = batch_size

        model_path = os.path.join(PROJECT_ROOT, self.word2vec_param.save_path)
        if not os.path.exists(model_path):
            logger.warning(f'Model {model_path} not found!! \n'
                           'Initializing new model ...')
            path = os.path.join(PROJECT_ROOT, '/'.join(str(
                self.word2vec_param.save_path).split('/')[:-1]))
            os.makedirs(path, exist_ok=True)

            # Save Word2Vec model
            real_data = self.real_samples.tolist()
            model = gensim.models.Word2Vec(
                sentences=real_data,
                vector_size=self.word2vec_param.vector_size,
                window=self.word2vec_param.window,
                min_count=self.word2vec_param.min_count,
                workers=self.word2vec_param.workers)
            model.save(model_path)

        logger.info('Loading Word2Vec model ...')
        self.model = gensim.models.Word2Vec.load(model_path)

    def embed(self, fake_sample):
        # Sample 100 real data samples
        idx = np.random.randint(0, self.real_samples.shape[0], self.batch_size)
        real_data_sampled = self.real_samples[idx]

        # Find average W2V embeddings for opcode sequences
        # from real and fake samples
        word2vec_embeddings_real = []
        for real_data in real_data_sampled:
            current_embedding = np.zeros(self.word2vec_param.vector_size,
                                         dtype=np.float64)
            for value in real_data:
                current_embedding += self.model.wv[value]
            word2vec_embeddings_real.append(current_embedding / len(real_data))

        word2vec_embeddings_fake = []
        for fake_data in fake_sample:
            current_embedding = np.zeros(self.word2vec_param.vector_size,
                                         dtype=np.float64)
            for value in fake_sample:
                current_embedding += self.model.wv[value]
            word2vec_embeddings_fake.append(
                current_embedding / len(fake_sample)
            )

        X = word2vec_embeddings_fake + word2vec_embeddings_real
        return np.array(X)


@hydra.main(config_path=str(PROJECT_ROOT / 'conf'), config_name='default')
def main(cfg: omegaconf.DictConfig):
    datamodule = hydra.utils.instantiate(
        cfg.data.datamodule, _recursive_=False
    )
    sample, num_unique = datamodule.load_data()

    model = hydra.utils.instantiate(
        cfg.word2vec.w2vmodule, real_samples=sample,
        batch_size=cfg.train.trainer.batch_size, _recursive_=False
    )
    print("Success!") if model else print("Fail!")


if __name__ == "__main__":
    logger.propagate = False
    main()
