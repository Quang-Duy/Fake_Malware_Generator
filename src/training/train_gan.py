""" Train script to train GAN and its varients

Usage

>>> python -m src.training.train_gan

"""

import logging
import os

import coloredlogs
import hydra
import matplotlib.pyplot as plt
import numpy as np
import omegaconf
from omegaconf import DictConfig

from src.common.utils import PROJECT_ROOT, MyTimer
from src.visualization.visualization import plot_losses

logging.getLogger('matplotlib').setLevel(logging.ERROR)
logging.getLogger('tensorflow').setLevel(logging.ERROR)
logging.getLogger('h5py').setLevel(logging.ERROR)

logger = logging.getLogger(__name__)
coloredlogs.install(level=logging.DEBUG, logger=logger)


def train(cfg: DictConfig, make_plot: bool = False) -> None:
    """Generic train loop"""
    logger.info('Loading dataset and rescale to [-1, 1] ...')
    datamodule = hydra.utils.instantiate(
        cfg.data.datamodule, _recursive_=False
    )
    X_train, num_unique = datamodule.load_data()
    factor = num_unique / 2
    X_train = X_train / factor - 1

    logger.info(f'Loading {cfg.model.modelmodule.model.name} model ...')
    gan_model = hydra.utils.instantiate(
        cfg.model.modelmodule, datamodule=cfg.data.datamodule,
        _recursive_=False
    )

    # Obtain parameters from hydra, prepare for training
    batch_size = cfg.train.trainer.batch_size
    latent_dim = cfg.model.modelmodule.model.latent_dim

    # Adversarial ground truths
    valid = gan_model.smooth_positive_labels(
        np.ones((batch_size, 1))
    )
    fake = np.zeros((batch_size, 1))

    # Save losses for each epoch for graphs
    results = {}

    my_timer = MyTimer()
    for epoch in range(cfg.train.trainer.epochs):
        # Obtain `batch_size` real data randomly
        idx = np.random.randint(low=0, high=X_train.shape[0], size=batch_size)
        real_data = X_train[idx]

        # Create noise vector as input for Generator
        noise = np.random.normal(0, 1, size=(batch_size, latent_dim))

        # Generate `batch_size` of fake data
        gen_samples = gan_model.generator.predict(noise)

        # ===Train Discriminator on real and fake data, separately ===#
        d_loss_real = gan_model.discriminator.train_on_batch(real_data, valid)
        d_loss_fake = gan_model.discriminator.train_on_batch(gen_samples, fake)

        # Compute Discriminator loss by averaging the 2 losses above
        d_loss = np.add(d_loss_real, d_loss_fake) / 2

        # Create noise vector as input for Network (combined G and D)
        noise = np.random.normal(0, 1, size=(batch_size, latent_dim))

        # === Train Generator === #
        g_loss = gan_model.network.train_on_batch(noise, valid)

        logger.debug('Epoch {} -- D_loss: {:.2f} -- D_real_loss: {:.2f} '
                     '-- D_fake_loss: {:.2f} -- G_loss: {:.2f}'
                     .format(epoch, d_loss[0], d_loss_real[0],
                             d_loss_fake[0], g_loss[0]))

        results[epoch] = [np.float64(d_loss[0]), np.float64(g_loss[0])]

        # Save model every `save_interval`
        if epoch % cfg.train.trainer.save_interval == 0:
            save_path = os.path.join(PROJECT_ROOT,
                                     cfg.train.trainer.save_path,
                                     'save_interval')
            file_name = str(cfg.model.modelmodule.model.name + '_' +
                            cfg.data.datamodule.datasets.train.family + '_' +
                            str(epoch) + '.hdf5')
            gan_model.save_model(save_path=save_path, file_name=file_name)

    logger.info('Finished training! '
                f'Execution time: {my_timer.get_execution_time()}')

    if make_plot:
        f_d, ax_d, f_g, ax_g = plot_losses(results, cfg.train.trainer.epochs)
        plt.legend(loc='upper left')
        f_d.savefig(os.path.join(
            save_path, cfg.model.modelmodule.model.name + '_discriminator.png')
        )
        f_g.savefig(os.path.join(
            save_path, cfg.model.modelmodule.model.name + '_generator.png'))
    logger.info('Finished operation!!')


@hydra.main(config_path=str(PROJECT_ROOT / 'conf'), config_name='default')
def main(cfg: omegaconf.DictConfig):
    train(cfg, make_plot=True)


if __name__ == '__main__':
    logger.propagate = False
    main()
