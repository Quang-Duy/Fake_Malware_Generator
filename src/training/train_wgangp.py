""" Train script to train WGAN-GP model

Usage

>>> python -m src.training.train_wgangp
# Run MlFlow UI to view results
>>> mlflow ui

"""

import logging
import os

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import coloredlogs
import hydra
import matplotlib.pyplot as plt
import mlflow
import tensorflow as tf
import numpy as np
import omegaconf
from omegaconf import DictConfig

from src.common.utils import PROJECT_ROOT, MyTimer
from src.visualization.visualization import plot_losses

logging.getLogger('git').setLevel(logging.ERROR)
logging.getLogger('urllib3').setLevel(logging.ERROR)
logging.getLogger('matplotlib').setLevel(logging.ERROR)
logging.getLogger('tensorflow').setLevel(logging.ERROR)
logging.getLogger('h5py').setLevel(logging.ERROR)

logger = logging.getLogger(__name__)
coloredlogs.install(level=logging.DEBUG, logger=logger)


def log_params(cfg: DictConfig) -> None:
    # Log parameters
    mlflow.log_params(cfg.data.datamodule.datasets.train)
    mlflow.log_params(cfg.train.trainer)
    for key, params in cfg.model.modelmodule.items():
        if key == '_target_' or key == 'discriminator_param':
            continue

        mlflow.log_params(params)

    for key, params in cfg.bert.bertmodule.items():
        if key == '_target_':
            continue
        mlflow.log_params(params)

    # Log hydra configs
    mlflow.log_artifacts(os.path.join(PROJECT_ROOT, 'conf'))

    # Log python scripts
    mlflow.log_artifact(
        os.path.join(PROJECT_ROOT, 'src/model/wgangp_model.py')
    )
    mlflow.log_artifact(
        os.path.join(PROJECT_ROOT, 'src/training/train_wgangp.py')
    )
    mlflow.log_artifact(
        os.path.join(PROJECT_ROOT, 'src/data/data_module.py')
    )


def train(cfg: DictConfig, make_plot: bool = False) -> None:
    """Generic train loop"""
    logger.info('Loading dataset ...')
    datamodule = hydra.utils.instantiate(
        cfg.data.datamodule, _recursive_=False
    )
    X_train = datamodule.prepare_data()
    # X_train = X_train[:64]

    distil_bert = hydra.utils.instantiate(
        cfg.bert.bertmodule, _recursive_=False
    )
    logger.info('Embedding dataset ...')
    X_train = distil_bert.embed(X_train)

    # Scale to [-1, 1]
    min_val = np.amin(X_train)
    max_val = np.amax(X_train)
    X_train = 2 * (X_train - min_val) / (max_val - min_val) - 1

    X_train = np.expand_dims(X_train, axis=2)

    logger.info(f'Initializing {cfg.model.modelmodule.model.model_name} '
                'model ...')
    wgangp_model = hydra.utils.instantiate(
        cfg.model.modelmodule,
        _recursive_=False
    )

    batch_size = cfg.train.trainer.batch_size
    latent_dim = cfg.model.modelmodule.model.latent_dim

    # Adversarial ground truths
    valid = np.ones((batch_size, 1))
    fake = -np.ones((batch_size, 1))
    dummy = np.zeros((batch_size, 1))  # Dummy gt for gradient penalty

    # Save losses for each epoch for graphs
    results = dict()

    mlflow.set_experiment(cfg.train.trainer.experiment_name)
    logger.info(f'Beginning experiment {cfg.train.trainer.experiment_name} ..')

    with mlflow.start_run(run_name=cfg.train.trainer.run_name):
        log_params(cfg)
        mlflow.log_param('Dataset_size', X_train.shape[0])

        my_timer = MyTimer()
        for epoch in range(cfg.train.trainer.epochs):
            logger.info(f'*** Epoch {epoch} ***')
            epoch_timer = MyTimer()
            for _ in range(cfg.model.modelmodule.critic_param.n_critic):
                critic_timer = MyTimer()
                # === Train critic === #
                # Obtain `batch_size` real data randomly
                idx = np.random.randint(
                    low=0, high=X_train.shape[0], size=batch_size
                )
                real_sample = X_train[idx]

                # Create noise vector as input for Generator
                noise = np.random.normal(0, 1, size=(batch_size, latent_dim))

                # Train critic
                c_loss = wgangp_model.critic_model.train_on_batch(
                    x=[real_sample, noise], y=[valid, fake, dummy]
                )
                if _ % 25 == 0:
                    logger.info(
                        f'Critic Iteration: {_} --- '
                        f'Critic Timer: {critic_timer.get_execution_time()}'
                    )

            # === Train Generator === #
            g_loss = wgangp_model.generator_model.train_on_batch(noise, valid)

            mlflow.log_metric('Critic_loss', c_loss[0], step=epoch)
            mlflow.log_metric('Gen_loss', g_loss, step=epoch)
            logger.debug('Epoch: {} -- Critic loss: {:.2f} -- Gen loss: {:.2f}'
                         .format(epoch, c_loss[0], g_loss))

            results[epoch] = [np.float64(c_loss[0]), np.float64(g_loss)]

            # Save model every `save_interval`
            if epoch % cfg.train.trainer.save_interval == 0:
                save_interval_folder = 'save_interval_bert'
                file_end_with = '_bert.hdf5'

                save_path = os.path.join(PROJECT_ROOT,
                                         cfg.train.trainer.save_path,
                                         save_interval_folder)
                file_name = str(cfg.model.modelmodule.model.model_name + '_' +
                                cfg.data.datamodule.datasets.train.family +
                                '_' + str(epoch) + file_end_with)
                wgangp_model.generator.save(save_path + '/' + file_name,
                                            include_optimizer=True,
                                            save_format='h5')
                mlflow.log_artifact(local_path=save_path + '/' + file_name)
            logger.info(
                f'Epoch Timer: {epoch_timer.get_execution_time()}'
            )

    logger.info('Finished training! '
                f'Execution time: {my_timer.get_execution_time()}')

    if make_plot:
        f_d, ax_d, f_g, ax_g = plot_losses(
            results, cfg.train.trainer.epochs,
            cfg.model.modelmodule.model.model_name
        )
        plt.legend(loc='upper left')
        f_d.savefig(os.path.join(
            save_path,
            cfg.model.modelmodule.model.model_name + '_critic.png')
        )
        f_g.savefig(os.path.join(
            save_path,
            cfg.model.modelmodule.model.model_name + '_generator.png'))
    logger.info('Finished operation!!')


@hydra.main(config_path=str(PROJECT_ROOT / 'conf'), config_name='default')
def main(cfg: omegaconf.DictConfig):
    train(cfg, make_plot=True)


if __name__ == '__main__':
    logger.propagate = False
    logger.info("Num GPUs Available: "
                f"{len(tf.config.list_physical_devices('GPU'))}")
    main()
