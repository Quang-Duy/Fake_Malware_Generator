""" Train script to train WGAN-GP model

Usage

>>> python -m src.training.train_wgangp

"""

import logging
import os

import coloredlogs
import hydra
import matplotlib.pyplot as plt
import numpy as np
import omegaconf
from omegaconf import DictConfig

from src.common.utils import PROJECT_ROOT, MyTimer
from src.visualization.visualization import plot_losses

logging.getLogger('matplotlib').setLevel(logging.ERROR)
logging.getLogger('tensorflow').setLevel(logging.ERROR)
logging.getLogger('h5py').setLevel(logging.ERROR)

logger = logging.getLogger(__name__)
coloredlogs.install(level=logging.DEBUG, logger=logger)


def train(cfg: DictConfig, make_plot: bool = False) -> None:
    """Generic train loop"""
    logger.info('Loading dataset and rescale to [-1, 1] ...')
    datamodule = hydra.utils.instantiate(
        cfg.data.datamodule, _recursive_=False
    )
    X_train, num_unique = datamodule.load_data()
    factor = num_unique / 2
    X_train = X_train / factor - 1
    X_train = np.expand_dims(X_train, axis=2)

    logger.info(f'Loading {cfg.model.modelmodule.model.name} model ...')
    wgangp_model = hydra.utils.instantiate(
        cfg.model.modelmodule, datamodule=cfg.data.datamodule,
        _recursive_=False
    )

    batch_size = cfg.train.trainer.batch_size
    latent_dim = cfg.model.modelmodule.model.latent_dim

    # Adversarial ground truths
    valid = np.ones((batch_size, 1))
    fake = -np.ones((batch_size, 1))
    dummy = np.zeros((batch_size, 1))  # Dummy gt for gradient penalty

    # Save losses for each epoch for graphs
    results = dict()

    my_timer = MyTimer()
    for epoch in range(cfg.train.trainer.epochs):
        for _ in range(cfg.model.modelmodule.critic_param.n_critic):
            # === Train critic === #
            # Obtain `batch_size` real data randomly
            idx = np.random.randint(low=0, high=X_train.shape[0],
                                    size=batch_size)
            real_sample = X_train[idx]

            # Create noise vector as input for Generator
            noise = np.random.normal(0, 1, size=(batch_size, latent_dim))

            # Train critic
            c_loss = wgangp_model.critic_model.train_on_batch(
                x=[real_sample, noise], y=[valid, fake, dummy]
            )

        # === Train Generator === #
        g_loss = wgangp_model.generator_model.train_on_batch(noise, valid)

        logger.debug('Epoch: {} -- Critic loss: {:.2f} -- Gen loss: {:.2f}'
                     .format(epoch, c_loss[0], g_loss))

        results[epoch] = [np.float64(c_loss[0]), np.float64(g_loss)]

        # Save model every `save_interval`
        if epoch % cfg.train.trainer.save_interval == 0:
            save_path = os.path.join(PROJECT_ROOT,
                                     cfg.train.trainer.save_path,
                                     'save_interval')
            file_name = str(cfg.model.modelmodule.model.name + '_' +
                            cfg.data.datamodule.datasets.train.family + '_' +
                            str(epoch) + '.hdf5')
            wgangp_model.generator.save(save_path + '/' + file_name,
                                        include_optimizer=True,
                                        save_format='h5')

    logger.info('Finished training! '
                f'Execution time: {my_timer.get_execution_time()}')

    if make_plot:
        f_d, ax_d, f_g, ax_g = plot_losses(results,
                                           cfg.train.trainer.epochs,
                                           cfg.model.modelmodule.model.name)
        plt.legend(loc='upper left')
        f_d.savefig(os.path.join(
            save_path,
            cfg.model.modelmodule.model.name + '_critic.png')
        )
        f_g.savefig(os.path.join(
            save_path,
            cfg.model.modelmodule.model.name + '_generator.png'))
    logger.info('Finished operation!!')


@hydra.main(config_path=str(PROJECT_ROOT / 'conf'), config_name='default')
def main(cfg: omegaconf.DictConfig):
    train(cfg, make_plot=True)


if __name__ == '__main__':
    logger.propagate = False
    main()