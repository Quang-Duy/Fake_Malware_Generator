""" Train script to train WGAN-GP model

Usage

>>> python -m src.training.train_wgangp
# Run MlFlow UI to view results
>>> mlflow ui

"""

import logging
import os

import coloredlogs
import mlflow
import hydra
import matplotlib.pyplot as plt
import numpy as np
import omegaconf
from omegaconf import DictConfig

from src.common.utils import PROJECT_ROOT, MyTimer
from src.visualization.visualization import plot_losses

logging.getLogger('git').setLevel(logging.ERROR)
logging.getLogger('urllib3').setLevel(logging.ERROR)
logging.getLogger('matplotlib').setLevel(logging.ERROR)
logging.getLogger('tensorflow').setLevel(logging.ERROR)
logging.getLogger('h5py').setLevel(logging.ERROR)

logger = logging.getLogger(__name__)
coloredlogs.install(level=logging.DEBUG, logger=logger)


def log_params(cfg: DictConfig) -> None:
    # Log parameters
    mlflow.log_params(cfg.data.datamodule.datasets.train)
    mlflow.log_params(cfg.train.trainer)
    for key, params in cfg.model.modelmodule.items():
        if (key == '_target_') or \
           (key == 'critic_param' and not cfg.train.trainer.is_bert):
            continue
        mlflow.log_params(params)

    for key, params in cfg.bert.bertmodule.items():
        if key == '_target_':
            continue
        mlflow.log_params(params)

    # Log hydra configs
    mlflow.log_artifacts(os.path.join(PROJECT_ROOT, 'conf'))

    # Log python scripts
    mlflow.log_artifact(
        os.path.join(PROJECT_ROOT, 'src/model/wgangp_model.py')
    )
    mlflow.log_artifact(
        os.path.join(PROJECT_ROOT, 'src/training/train_wgangp.py')
    )
    mlflow.log_artifact(
        os.path.join(PROJECT_ROOT, 'src/data/data_module.py')
    )


def train(cfg: DictConfig, make_plot: bool = False) -> None:
    """Generic train loop"""
    tokenized = not cfg.train.trainer.is_bert
    logger.info('Loading dataset and rescale to [-1, 1] ...')
    datamodule = hydra.utils.instantiate(
        cfg.data.datamodule, _recursive_=False
    )
    X_train, num_unique = datamodule.load_data(tokenized=tokenized)
    X_train = X_train[:64, :]

    if not cfg.train.trainer.is_bert:
        factor = num_unique / 2
        X_train = X_train / factor - 1

    if cfg.train.trainer.is_bert:
        distil_bert = hydra.utils.instantiate(
            cfg.bert.bertmodule, _recursive_=False
        )
        X_train = distil_bert.embed(X_train)

    X_train = np.expand_dims(X_train, axis=2)

    logger.info(f'Initializing {cfg.model.modelmodule.model.model_name} model '
                f'...')
    wgangp_model = hydra.utils.instantiate(
        cfg.model.modelmodule, datamodule=cfg.data.datamodule,
        _recursive_=False
    )

    batch_size = cfg.train.trainer.batch_size
    latent_dim = cfg.model.modelmodule.model.latent_dim

    # Adversarial ground truths
    valid = np.ones((batch_size, 1))
    fake = -np.ones((batch_size, 1))
    dummy = np.zeros((batch_size, 1))  # Dummy gt for gradient penalty

    # Save losses for each epoch for graphs
    results = dict()

    mlflow.set_experiment(cfg.train.trainer.experiment_name)
    logger.info(f'Beginning experiment {cfg.train.trainer.experiment_name} ..')

    my_timer = MyTimer()
    with mlflow.start_run(run_name=cfg.train.trainer.run_name) as run:
        log_params(cfg)
        mlflow.log_param('Dataset_size', X_train.shape[0])
        for epoch in range(cfg.train.trainer.epochs):
            for _ in range(cfg.model.modelmodule.critic_param.n_critic):
                # === Train critic === #
                # Obtain `batch_size` real data randomly
                idx = np.random.randint(low=0, high=X_train.shape[0],
                                        size=batch_size)
                real_sample = X_train[idx]

                # Create noise vector as input for Generator
                noise = np.random.normal(0, 1, size=(batch_size, latent_dim))

                # Train critic
                c_loss = wgangp_model.critic_model.train_on_batch(
                    x=[real_sample, noise], y=[valid, fake, dummy]
                )

            # === Train Generator === #
            g_loss = wgangp_model.generator_model.train_on_batch(noise, valid)

            mlflow.log_metric('Critic_loss', c_loss[0], step=epoch)
            mlflow.log_metric('Gen_loss', g_loss, step=epoch)
            logger.debug('Epoch: {} -- Critic loss: {:.2f} -- Gen loss: {:.2f}'
                         .format(epoch, c_loss[0], g_loss))

            results[epoch] = [np.float64(c_loss[0]), np.float64(g_loss)]

            # Save model every `save_interval`
            if epoch % cfg.train.trainer.save_interval == 0:
                save_interval_folder = 'save_interval' \
                    if not cfg.train.trainer.is_bert else 'save_interval_bert'
                file_end_with = '.hdf5' if not cfg.train.trainer.is_bert \
                    else '_bert.hdf5'

                save_path = os.path.join(PROJECT_ROOT,
                                         cfg.train.trainer.save_path,
                                         save_interval_folder)
                file_name = str(cfg.model.modelmodule.model.model_name + '_' +
                                cfg.data.datamodule.datasets.train.family +
                                '_' + str(epoch) + file_end_with)
                wgangp_model.generator.save(save_path + '/' + file_name,
                                            include_optimizer=True,
                                            save_format='h5')
                mlflow.log_artifact(local_path=save_path + '/' + file_name)

    logger.info('Finished training! '
                f'Execution time: {my_timer.get_execution_time()}')

    if make_plot:
        f_d, ax_d, f_g, ax_g = plot_losses(
            results, cfg.train.trainer.epochs,
            cfg.model.modelmodule.model.model_name
        )
        plt.legend(loc='upper left')
        f_d.savefig(os.path.join(
            save_path,
            cfg.model.modelmodule.model.model_name + '_critic.png')
        )
        f_g.savefig(os.path.join(
            save_path,
            cfg.model.modelmodule.model.model_name + '_generator.png'))
    logger.info('Finished operation!!')


@hydra.main(config_path=str(PROJECT_ROOT / 'conf'), config_name='default')
def main(cfg: omegaconf.DictConfig):
    train(cfg, make_plot=True)


if __name__ == '__main__':
    logger.propagate = False
    main()
