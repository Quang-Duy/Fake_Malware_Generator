import logging
import coloredlogs
import hydra
import numpy as np
import mlflow
import os
import omegaconf
from ruamel.yaml import YAML
from omegaconf import OmegaConf, DictConfig
from src.common.utils import PROJECT_ROOT, MyTimer

logger = logging.getLogger(__name__)
coloredlogs.install(level=logging.DEBUG, logger=logger)

logging.getLogger('git').setLevel(logging.ERROR)
logging.getLogger('urllib3').setLevel(logging.ERROR)
logging.getLogger('tensorflow').setLevel(logging.ERROR)
logging.getLogger('h5py').setLevel(logging.ERROR)

yaml = YAML()
yaml.preserve_quotes = True
yaml.boolean_representation = ['False', 'True']

logger.propagate = False


def log_params(cfg: DictConfig) -> None:
    # Log Hydra configs parameters
    mlflow.log_params(cfg.data.datamodule.datasets.train)

    # Log hydra config files
    mlflow.log_artifacts(os.path.join(PROJECT_ROOT, 'conf'))

    # Log python scripts
    mlflow.log_artifact(
        os.path.join(PROJECT_ROOT, 'src/tuning/tuning_classifiers.py')
    )
    mlflow.log_artifact(
        os.path.join(PROJECT_ROOT, 'src/data/data_module.py')
    )


def update_parameters(best_params, model_name) -> None:
    param_file = './conf/classifiers/default.yaml'
    with open(param_file) as f:
        doc = yaml.load(f)

    clf_param = None
    if model_name == 'OneClassSVM':
        clf_param = 'ocsvm_param'
    elif model_name == 'IsolationForest':
        clf_param = 'isoForest_param'
    elif model_name == 'LocalOutlierFactor':
        clf_param = 'lof_param'
    else:
        raise TypeError('Model does not belong to Classifiers class')

    for param, value in best_params.items():
        doc['clf_module'][clf_param][param] = value

    with open(param_file, 'w') as f:
        yaml.dump(doc, f)


def tune_classifier(cfg: omegaconf.DictConfig, X_train, y_train):
    clf = hydra.utils.instantiate(
        cfg.classifiers.clf_module,
        _recursive_=False
    )

    logger.info('Loading Classifier ...')
    model = clf.select_classifier()
    model_name = model.__class__.__name__

    os.chdir(PROJECT_ROOT)
    mlflow.set_experiment(cfg.optim.optim_module.optim_experiment_name)
    logger.info('Beginning experiment '
                f'{cfg.optim.optim_module.optim_experiment_name} ...')

    param_grid = None
    with mlflow.start_run(run_name=cfg.optim.optim_module.optim_run_name):
        log_params(cfg)
        mlflow.log_param(key='classifier_name', value=model_name)

        if model_name == 'OneClassSVM':
            param_grid = OmegaConf.to_container(cfg.optim.optim_module.ocsvm)
            mlflow.log_params(cfg.optim.optim_module.ocsvm)

        elif model_name == 'IsolationForest':
            param_grid = OmegaConf.to_container(
                cfg.optim.optim_module.isolation_forest
            )
            mlflow.log_params(cfg.optim.optim_module.isolation_forest)

        elif model_name == 'LocalOutlierFactor':
            param_grid = OmegaConf.to_container(cfg.optim.optim_module.lof)
            mlflow.log_params(cfg.optim.optim_module.lof)
        else:
            raise TypeError('Model does not belong to Classifiers class')

        if not param_grid:
            raise ModuleNotFoundError(
                f'Fine-tuning parameters for {model_name} not found! '
                'Check conf/optim/default.yaml'
            )

        logger.info(f'Tuning {model_name} ...')
        my_timer = MyTimer()
        best_param = clf.tuning_parameters(
            clf=model, param_grid=param_grid, X_train=X_train, y_train=y_train
        )
        logger.debug(f'Best Parameters for {model_name}: {best_param}')
        for param_type, param_value in best_param.items():
            mlflow.log_param(key=f'best_{param_type}', value=str(param_value))

        logger.info('Finished fine-tuning! '
                    f'Execution time: {my_timer.get_execution_time()}')

        update_parameters(best_params=best_param, model_name=model_name)


@hydra.main(config_path=str(PROJECT_ROOT / 'conf'), config_name='default')
def main(cfg: omegaconf.DictConfig):
    logger.info('Loading dataset ...')
    datamodule = hydra.utils.instantiate(
        cfg.data.datamodule, _recursive_=False
    )
    X_train = datamodule.prepare_data()
    X_train = X_train[:64]

    distil_bert = hydra.utils.instantiate(
        cfg.bert.bertmodule, _recursive_=False
    )
    X_train = datamodule.embed_data(data=X_train, distil_bert=distil_bert)

    # Create y_train for tuning hyperparameters for classifiers
    y_train = np.array([1] * len(X_train))

    # Tune classifier
    tune_classifier(cfg=cfg, X_train=X_train, y_train=y_train)


if __name__ == "__main__":
    logger.propagate = False
    main()
